from pyspark.sql import functions as F
from pyspark.sql.functions import datediff, current_date, expr

class FeatureEngineeringPipelineSpark:
    def __init__(self, spark_df):
        self.df = spark_df
    
    def preprocess(self):
        # Convert dates from string to date type if necessary
        self.df = self.df.withColumn("business_date", F.col("business_date").cast("date"))
        self.df = self.df.withColumn("open_date", F.col("open_date").cast("date"))
        self.df = self.df.withColumn("close_date", F.col("close_date").cast("date"))
        return self
    
    def add_account_longevity(self):
        # Assuming `open_date` is the account opening date
        self.df = self.df.withColumn("account_longevity", datediff(current_date(), F.col("open_date")))
        return self
    
    def add_balance_features(self):
        # Calculate mean, max, and min balances (example with curr_bal_amt)
        self.df = self.df.withColumn("curr_bal_amt", F.col("curr_bal_amt").cast("float"))  # Ensure column is float type
        balance_stats = self.df.groupBy("hh_id_in_wh").agg(
            F.mean("curr_bal_amt").alias("curr_bal_amt_mean"),
            F.max("curr_bal_amt").alias("curr_bal_amt_max"),
            F.min("curr_bal_amt").alias("curr_bal_amt_min")
        )
        self.df = self.df.join(balance_stats, "hh_id_in_wh", "left")
        return self
    
    def add_product_diversity(self):
        # Count distinct products used by hh_id_in_wh
        product_diversity = self.df.groupBy("hh_id_in_wh").agg(
            F.countDistinct("prd_code").alias("product_diversity")
        )
        self.df = self.df.join(product_diversity, "hh_id_in_wh", "left")
        return self
    
    def execute(self):
        self.preprocess()
        self.add_account_longevity()
        self.add_balance_features()
        self.add_product_diversity()
        # Add more feature engineering methods as needed
        return self.df




def load_pwm_clients(spark, reference_date='2024-04-08'):
    """
    Loads the IDs of clients who are currently private wealth management (PWM) clients as of a given date.

    Args:
        spark (SparkSession): The Spark session object.
        reference_date (str): The date to filter PWM clients.

    Returns:
        DataFrame: A DataFrame containing distinct hh_id_in_wh for PWM clients.
    """
    query = f"""
    SELECT DISTINCT hh_id_in_wh 
    FROM dm_r3.pwm_mstr_dtl_daily 
    WHERE business_date = '{reference_date}' AND seg_code = 'PWM'
    """
    return spark.sql(query)





from pyspark.sql.functions import col, stddev, mean

def normalize_features(df, features):
    """
    Normalize the specified features of a DataFrame to have zero mean and unit variance.

    Args:
        df (DataFrame): The Spark DataFrame to be normalized.
        features (list of str): List of column names to normalize.

    Returns:
        DataFrame: The DataFrame with normalized features.
    """
    for feature in features:
        df = df.withColumn(feature, (col(feature) - mean(feature).over(Window.partitionBy())) / stddev(feature).over(Window.partitionBy()))
    return df



from pyspark.sql.functions import col, sqrt, sum as sql_sum
from pyspark.sql.window import Window

def calculate_similarity(spark, non_pwm_df, pwm_df, features):
    """
    Calculates cosine similarity between non-PWM and PWM client feature vectors.

    Args:
        spark (SparkSession): The Spark session object.
        non_pwm_df (DataFrame): DataFrame containing non-PWM clients.
        pwm_df (DataFrame): DataFrame containing PWM clients.
        features (list of str): List of feature columns used for similarity calculation.

    Returns:
        DataFrame: A DataFrame with hh_id_in_wh and their maximum similarity scores.
    """
    # Normalize features in both DataFrames
    non_pwm_df = normalize_features(non_pwm_df, features)
    pwm_df = normalize_features(pwm_df, features)

    # Cross join PWM and non-PWM clients (Optimize this step based on the data size)
    cross_df = non_pwm_df.crossJoin(pwm_df.select(col("hh_id_in_wh").alias("pwm_hh_id_in_wh"), *features))

    # Calculate dot products for cosine similarity
    dot_products = sum((col(f) * col(f"pwm_{f}")) for f in features)
    norms = sqrt(sum(col(f)**2 for f in features))

    cross_df = cross_df.withColumn("cosine_similarity", dot_products / (norms * norms))

    # Calculate the maximum similarity for each non-PWM client
    windowSpec = Window.partitionBy("hh_id_in_wh")
    result_df = cross_df.withColumn("max_similarity", sql_sum("cosine_similarity").over(windowSpec))

    return result_df.select("hh_id_in_wh", "max_similarity").distinct()


