from pyspark.sql import functions as F
from pyspark.sql.functions import datediff, current_date, expr

class FeatureEngineeringPipelineSpark:
    def __init__(self, spark_df):
        self.df = spark_df
    
    def preprocess(self):
        # Convert dates from string to date type if necessary
        self.df = self.df.withColumn("business_date", F.col("business_date").cast("date"))
        self.df = self.df.withColumn("open_date", F.col("open_date").cast("date"))
        self.df = self.df.withColumn("close_date", F.col("close_date").cast("date"))
        return self
    
    def add_account_longevity(self):
        # Assuming `open_date` is the account opening date
        self.df = self.df.withColumn("account_longevity", datediff(current_date(), F.col("open_date")))
        return self
    
    def add_balance_features(self):
        # Calculate mean, max, and min balances (example with curr_bal_amt)
        self.df = self.df.withColumn("curr_bal_amt", F.col("curr_bal_amt").cast("float"))  # Ensure column is float type
        balance_stats = self.df.groupBy("hh_id_in_wh").agg(
            F.mean("curr_bal_amt").alias("curr_bal_amt_mean"),
            F.max("curr_bal_amt").alias("curr_bal_amt_max"),
            F.min("curr_bal_amt").alias("curr_bal_amt_min")
        )
        self.df = self.df.join(balance_stats, "hh_id_in_wh", "left")
        return self
    
    def add_product_diversity(self):
        # Count distinct products used by hh_id_in_wh
        product_diversity = self.df.groupBy("hh_id_in_wh").agg(
            F.countDistinct("prd_code").alias("product_diversity")
        )
        self.df = self.df.join(product_diversity, "hh_id_in_wh", "left")
        return self
    
    def execute(self):
        self.preprocess()
        self.add_account_longevity()
        self.add_balance_features()
        self.add_product_diversity()
        # Add more feature engineering methods as needed
        return self.df


